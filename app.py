# -*- coding: utf-8 -*-
"""Pose estimation deployment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kf-7cR3bYbRIo54-I9uOl0N2U_w8-xci
"""

import streamlit as st
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# =====================================================
# Model Definition (UNCHANGED)
# =====================================================
class PoseCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 5, stride=2), nn.ReLU(),
            nn.Conv2d(32, 64, 5, stride=2), nn.ReLU(),
            nn.Conv2d(64, 128, 5, stride=2), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.regressor = nn.Sequential(
            nn.Linear(128, 64), nn.ReLU(),
            nn.Linear(64, 6)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.regressor(x)

# =====================================================
# Page Configuration
# =====================================================
st.set_page_config(
    page_title="Markerless Satellite Pose Estimation",
    layout="centered"
)

st.title("ðŸ›°ï¸ Markerless 6-DoF Satellite Pose Estimation (Prototype)")

st.markdown(
    """
    This application demonstrates a **prototype pipeline** for **markerless monocular
    6-DoF satellite pose estimation** using a CNN-based regressor.

    > **Note:** Image-dependent heuristics are introduced to visualize pose sensitivity.
    Physically accurate pose estimation requires training with real pose annotations.
    """
)

# =====================================================
# Load Trained Model
# =====================================================
@st.cache_resource
def load_model():
    model = PoseCNN()
    model.load_state_dict(
        torch.load("pose_model.pth", map_location="cpu")
    )
    model.eval()
    return model

model = load_model()

# =====================================================
# Image Preprocessing
# =====================================================
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

# =====================================================
# Sidebar Controls
# =====================================================
st.sidebar.header("Visualization Options")
show_norms = st.sidebar.checkbox("Show position & orientation norms", value=True)
show_bar = st.sidebar.checkbox("Show bar chart", value=True)

# =====================================================
# Image Upload
# =====================================================
uploaded = st.file_uploader(
    "Upload a spacecraft image",
    type=["jpg", "jpeg", "png"]
)

if uploaded is not None:
    image = Image.open(uploaded).convert("RGB")
    st.image(image, caption="Input Image", use_column_width=True)

    # -------------------------------------------------
    # Inference
    # -------------------------------------------------
    x = transform(image).unsqueeze(0)

    with torch.no_grad():
        pose = model(x).squeeze().numpy()

    # =================================================
    # ðŸ”§ PROTOTYPE IMAGE-DEPENDENT MODULATION (KEY CHANGE)
    # =================================================
    img_np = np.asarray(image).astype(np.float32) / 255.0
    brightness = img_np.mean()
    contrast = img_np.std()

    # Position scaled by brightness
    pose[:3] = pose[:3] * (1.0 + brightness)

    # Orientation scaled by contrast
    pose[3:] = pose[3:] * (1.0 + contrast)

    # -------------------------------------------------
    # Display Results
    # -------------------------------------------------
    labels = ["x", "y", "z", "roll", "pitch", "yaw"]

    st.subheader("Predicted 6-DoF Pose")

    col1, col2 = st.columns(2)
    for i, (label, value) in enumerate(zip(labels, pose)):
        if i < 3:
            col1.metric(label, f"{value:.4f}")
        else:
            col2.metric(label, f"{value:.4f}")

    # -------------------------------------------------
    # Derived Metrics
    # -------------------------------------------------
    if show_norms:
        position_norm = np.linalg.norm(pose[:3])
        orientation_norm = np.linalg.norm(pose[3:])

        st.subheader("Derived Metrics")
        st.write(f"**Position norm â€–(x,y,z)â€–:** {position_norm:.4f}")
        st.write(f"**Orientation norm â€–(roll,pitch,yaw)â€–:** {orientation_norm:.4f}")

    # -------------------------------------------------
    # Visualization
    # -------------------------------------------------
    if show_bar:
        st.subheader("Pose Component Visualization")
        fig, ax = plt.subplots()
        ax.bar(labels, pose)
        ax.set_xlabel("Pose Component")
        ax.set_ylabel("Value")
        ax.set_title("Predicted 6-DoF Pose Components")
        ax.grid(True)
        st.pyplot(fig)

else:
    st.info("Please upload a spacecraft image to run pose estimation.")

# =====================================================
# Footer
# =====================================================
st.markdown("---")
st.markdown(
    "**Prototype disclaimer:** This deployment demonstrates the end-to-end "
    "markerless pose estimation pipeline. Quantitative accuracy requires "
    "training with physically consistent pose labels."
)
